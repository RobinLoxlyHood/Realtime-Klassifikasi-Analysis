{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import multiprocessing\n",
    "import io\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_metrics as km\n",
    "import pickle\n",
    "import keras\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM, Bidirectional, Dropout, GRU\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from gensim.models import word2vec\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score, multilabel_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from varname import nameof\n",
    "from numpy import savetxt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import wget\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "import geopandas as geop\n",
    "\n",
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pak @ganjarpranowo ... Sangat memalukan sekali...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@cahedewe000 @ChusnulCh__ @arsul_sani @ganjarp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@amr715882 @gibran_tweet @jokowi @ListyoSigitP...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@puspita3_eka @aniesbaswedan @Mars_Sahsa @ganj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Catatan_ali7 @amr715882 @gibran_tweet @jokowi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  Pak @ganjarpranowo ... Sangat memalukan sekali...      0\n",
       "1  @cahedewe000 @ChusnulCh__ @arsul_sani @ganjarp...      0\n",
       "2  @amr715882 @gibran_tweet @jokowi @ListyoSigitP...      0\n",
       "3  @puspita3_eka @aniesbaswedan @Mars_Sahsa @ganj...      0\n",
       "4  @Catatan_ali7 @amr715882 @gibran_tweet @jokowi...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"dataset/Dataset.csv\", sep=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#menyimpan tweet. (tipe data series pandas)\n",
    "data_content = df['tweet']\n",
    "# casefolding\n",
    "data_casefolding = data_content.str.lower()\n",
    "#data_casefolding.head()\n",
    "#filtering\n",
    "\n",
    "#url\n",
    "filtering_url = [re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", tweet) for tweet in data_casefolding]\n",
    "#cont\n",
    "filtering_cont = [re.sub(r'\\(cont\\)',\" \", tweet)for tweet in filtering_url]\n",
    "#punctuatuion\n",
    "filtering_punctuation = [re.sub('[!\"”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', tweet) for tweet in filtering_cont]  #hapus simbol'[!#?,.:\";@()-_/\\']'\n",
    "#  hapus #tagger\n",
    "filtering_tagger = [re.sub(r'#([^\\s]+)', '', tweet) for tweet in filtering_punctuation]\n",
    "#numeric\n",
    "filtering_numeric = [re.sub(r'\\d+', ' ', tweet) for tweet in filtering_tagger]\n",
    "\n",
    "# # filtering RT , @ dan #\n",
    "# fungsi_clen_rt = lambda x: re.compile('\\#').sub('', re.compile('rt @').sub('@', x, count=1).strip())\n",
    "# clean = [fungsi_clen_rt for tweet in filtering_numeric]\n",
    "\n",
    "data_filtering = pd.Series(filtering_numeric)\n",
    "\n",
    "# #tokenize\n",
    "tknzr = TweetTokenizer()\n",
    "data_tokenize = [tknzr.tokenize(tweet) for tweet in data_filtering]\n",
    "\n",
    "#slang word\n",
    "path_dataslang = open(\"dataset/kamus kata baku-clear (1).csv\")\n",
    "dataslang = pd.read_csv(path_dataslang, encoding = 'utf-8', header=None, sep=\";\")\n",
    "\n",
    "def replaceSlang(word):\n",
    "  if word in list(dataslang[0]):\n",
    "    indexslang = list(dataslang[0]).index(word)\n",
    "    return dataslang[1][indexslang]\n",
    "  else:\n",
    "    return word\n",
    "\n",
    "data_formal = []\n",
    "for data in data_tokenize:\n",
    "  data_clean = [replaceSlang(word) for word in data]\n",
    "  data_formal.append(data_clean)\n",
    "len_data_formal = len(data_formal)\n",
    "# print(data_formal)\n",
    "# len_data_formal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iki11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "default_stop_words = nltk.corpus.stopwords.words('indonesian')\n",
    "stopwords = set(default_stop_words)\n",
    "\n",
    "def removeStopWords(line, stopwords):\n",
    "  words = []\n",
    "  for word in line:  \n",
    "    word=str(word)\n",
    "    word = word.strip()\n",
    "    if word not in stopwords and word != \"\" and word != \"&\":\n",
    "      words.append(word)\n",
    "\n",
    "  return words\n",
    "data_notstopword = [removeStopWords(line,stopwords) for line in data_formal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_list = [\"bali\"] #ini perlu/tidak perlu diubah karena dianggap sastrawi sebagai imbuhan i\n",
    "\n",
    "factory = StemmerFactory()\n",
    "ind_stemmer = factory.create_stemmer()\n",
    "def stemmer(line):\n",
    "    temp = list()\n",
    "    for word in line:\n",
    "      if(word not in white_list):\n",
    "        word = ind_stemmer.stem(word)\n",
    "      if(len(word)>3):\n",
    "        temp.append(word)\n",
    "    return temp\n",
    "\n",
    "reviews = [stemmer (line) for line in data_notstopword]\n",
    "# print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah index :  3305 \n",
      "\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "#Pembuatan Kamus kata\n",
    "t  = Tokenizer()\n",
    "fit_text = reviews\n",
    "t.fit_on_texts(fit_text)\n",
    "\n",
    "#Pembuatan Id masing-masing kata\n",
    "sequences = t.texts_to_sequences(reviews)\n",
    "\n",
    "#hapus duplikat kata yang muncul\n",
    "list_set_sequence = [list(dict.fromkeys(seq)) for seq in sequences]\n",
    "\n",
    "#mencari max length sequence\n",
    "def FindMaxLength(lst): \n",
    "    maxList = max((x) for x in lst) \n",
    "    maxLength = max(len(x) for x in lst ) \n",
    "    return maxList, maxLength \n",
    "      \n",
    "# Driver Code \n",
    "max_seq, max_length_seq = FindMaxLength(list_set_sequence)\n",
    "jumlah_index = len(t.word_index) +1\n",
    "\n",
    "print('jumlah index : ',jumlah_index,'\\n')\n",
    "# print('word_index : ',t.word_index,'\\n')\n",
    "# print('index kalimat asli     : ', sequences,'\\n')\n",
    "# print('kalimat tanpa duplikat : ',list_set_sequence,'\\n')\n",
    "# print('panjang max kalimat : ', max_length_seq,'kata','\\n')\n",
    "# print('kalimat terpanjang setelah dihapus duplikat : ', max_seq,'\\n')\n",
    "\n",
    "count_word = [len(i) for i in list_set_sequence]\n",
    "# print('list panjang kalimat : ', count_word)\n",
    "max_len_word = max(count_word)\n",
    "print(max_len_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding= pad_sequences([list(list_set_sequence[i]) for i in range(len(list_set_sequence))], \n",
    "                       maxlen= 50, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1802, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsiSentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5a45b1134addb48ce7f7d339bb60d17128da9b0792c8350f1ab497b8b34fd0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
