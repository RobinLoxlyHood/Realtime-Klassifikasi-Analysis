{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HI4ujUlnZah4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import multiprocessing\n",
        "import io\n",
        "import gensim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import keras\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Activation, Embedding, LSTM, Bidirectional, Dropout, GRU\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import defaultdict\n",
        "from datetime import timedelta\n",
        "from gensim.models import word2vec\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from varname import nameof\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtyarzfKc1DO"
      },
      "source": [
        "#2.Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zg_Jvs3Qa3pq"
      },
      "outputs": [],
      "source": [
        "#simpan path dataset\n",
        "path_data_aspek = \"data_scraaping_kotaTegal_Labeling.csv\"\n",
        " \n",
        "#read dataset\n",
        "data_aspek = pd.read_csv(path_data_aspek, sep=\";\", header=[0], encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHd8zoL5ePma"
      },
      "source": [
        "#3.Implementasi Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LcNrqHr50MeU"
      },
      "outputs": [],
      "source": [
        "#menyimpan tweet. (tipe data series pandas)\n",
        "data_content = data_aspek['tweet']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNzxbK5i0Pcu"
      },
      "source": [
        "##3.1. Casefolding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALc2jvPU0Ueb",
        "outputId": "b689ceab-a9c9-4e80-87ec-f8515f3a338d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [tf] yg jual gini area slawi aja dimana ya ges...\n",
              "1    [tf] spill game offline kalian yg menarik itu ...\n",
              "2    [tf] cara hapus tag contact di get contact yan...\n",
              "3    selamat bertugas kepada peserta bsi explore ya...\n",
              "4    minggu, 15 januari 2023 petugas pleton c regu ...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# casefolding\n",
        "data_casefolding = data_content.str.lower()\n",
        "data_casefolding.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A0nfLDO0Xgy"
      },
      "source": [
        "##3.2. Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pVsCbmmq0cuS"
      },
      "outputs": [],
      "source": [
        "#filtering\n",
        "\n",
        "#url\n",
        "filtering_url = [re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", tweet) for tweet in data_casefolding]\n",
        "#cont\n",
        "filtering_cont = [re.sub(r'\\(cont\\)',\" \", tweet)for tweet in filtering_url]\n",
        "#punctuatuion\n",
        "filtering_punctuation = [re.sub('[!\"”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', tweet) for tweet in filtering_cont]  #hapus simbol'[!#?,.:\";@()-_/\\']'\n",
        "#  hapus #tagger\n",
        "filtering_tagger = [re.sub(r'#([^\\s]+)', '', tweet) for tweet in filtering_punctuation]\n",
        "#numeric\n",
        "filtering_numeric = [re.sub(r'\\d+', ' ', tweet) for tweet in filtering_tagger]\n",
        "\n",
        "# # filtering RT , @ dan #\n",
        "# fungsi_clen_rt = lambda x: re.compile('\\#').sub('', re.compile('rt @').sub('@', x, count=1).strip())\n",
        "# clean = [fungsi_clen_rt for tweet in filtering_numeric]\n",
        "\n",
        "data_filtering = pd.Series(filtering_numeric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts2lHtue0kAk"
      },
      "source": [
        "## 3.3. Tokenisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2sooKgeb0o9R"
      },
      "outputs": [],
      "source": [
        "# #tokenize\n",
        "tknzr = TweetTokenizer()\n",
        "data_tokenize = [tknzr.tokenize(tweet) for tweet in data_filtering]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZk9kLGa0sSB"
      },
      "source": [
        "##3.4. Konversi Slangword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LXnSpBqGeGyb",
        "outputId": "9452a382-b40a-4743-bf03-f2d075b6d1e4"
      },
      "outputs": [],
      "source": [
        "#slang word\n",
        "path_dataslang = open(\"kamus kata baku-clear (1).csv\")\n",
        "dataslang = pd.read_csv(path_dataslang, encoding = 'utf-8', header=None, sep=\";\")\n",
        "\n",
        "def replaceSlang(word):\n",
        "  if word in list(dataslang[0]):\n",
        "    indexslang = list(dataslang[0]).index(word)\n",
        "    return dataslang[1][indexslang]\n",
        "  else:\n",
        "    return word\n",
        "\n",
        "data_formal = []\n",
        "for data in data_tokenize:\n",
        "  data_clean = [replaceSlang(word) for word in data]\n",
        "  data_formal.append(data_clean)\n",
        "len_data_formal = len(data_formal)\n",
        "# print(data_formal)\n",
        "# len_data_formal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "RdF1uHjxLWie",
        "outputId": "b6c8f63b-4323-4e06-f66a-37c409acc883"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tlg</td>\n",
              "      <td>tolong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dwpan</td>\n",
              "      <td>depan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dwngan</td>\n",
              "      <td>dengan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>byk</td>\n",
              "      <td>banyak</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>smoga</td>\n",
              "      <td>semoga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5437</th>\n",
              "      <td>politic</td>\n",
              "      <td>politik</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5438</th>\n",
              "      <td>cont</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5439</th>\n",
              "      <td>rt</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5440</th>\n",
              "      <td>cc</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5441</th>\n",
              "      <td>huhuhuhuhuhuhuhuhuhuuuuuuuuuuuuuu</td>\n",
              "      <td>sedih</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5442 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      0        1\n",
              "0                                   tlg   tolong\n",
              "1                                 dwpan    depan\n",
              "2                                dwngan   dengan\n",
              "3                                   byk   banyak\n",
              "4                                 smoga   semoga\n",
              "...                                 ...      ...\n",
              "5437                            politic  politik\n",
              "5438                               cont         \n",
              "5439                                 rt         \n",
              "5440                                 cc      NaN\n",
              "5441  huhuhuhuhuhuhuhuhuhuuuuuuuuuuuuuu    sedih\n",
              "\n",
              "[5442 rows x 2 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataslang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp6NXGRerCK4"
      },
      "source": [
        "## 3.5. Stopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogP3tGEHggHJ",
        "outputId": "9a39c860-ccdd-4456-fe22-3bc54ab0e910"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\iki11\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "default_stop_words = nltk.corpus.stopwords.words('indonesian')\n",
        "stopwords = set(default_stop_words)\n",
        "\n",
        "def removeStopWords(line, stopwords):\n",
        "  words = []\n",
        "  for word in line:  \n",
        "    word=str(word)\n",
        "    word = word.strip()\n",
        "    if word not in stopwords and word != \"\" and word != \"&\":\n",
        "      words.append(word)\n",
        "\n",
        "  return words\n",
        "reviews = [removeStopWords(line,stopwords) for line in data_formal]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONUwUmSe-cBd",
        "outputId": "9cc238d2-51f8-4c58-ec64-580895c18ffb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYTxiKe1Qv0"
      },
      "source": [
        "# 4.Implementasi Naive Bayes (Klasifikasi Aspek)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fa49U0myBhg"
      },
      "source": [
        "## 4.1.pembuatan vector kata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJyFVF8w1UX4",
        "outputId": "775ed408-4ba1-41f0-f04c-3e9b879e4c0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1012, 3852)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pembuatan vector kata\n",
        "vectorizer = TfidfVectorizer()\n",
        "reviews2 = [\" \".join(r) for r in reviews]\n",
        "vektor_tfidf = vectorizer.fit_transform(reviews2)\n",
        "vektor_tfidf = vektor_tfidf.toarray()\n",
        "vektor_tfidf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx_qAytn5v6d",
        "outputId": "6ae1e3a9-cc9c-4e8f-f507-9fe776d768d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(vektor_tfidf)\n",
        "# print(vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEAIKfy2x5OE"
      },
      "source": [
        "## 4.2.split data dan cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>username</th>\n",
              "      <th>tweet</th>\n",
              "      <th>wisata/hiburan</th>\n",
              "      <th>pendidikan</th>\n",
              "      <th>fasilitas/layanan publik</th>\n",
              "      <th>kuliner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16/01/2023 10:08</td>\n",
              "      <td>tegalfess</td>\n",
              "      <td>[tf] yg jual gini area slawi aja dimana ya ges...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16/01/2023 09:56</td>\n",
              "      <td>tegalfess</td>\n",
              "      <td>[tf] spill game offline kalian yg menarik itu ...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16/01/2023 09:47</td>\n",
              "      <td>tegalfess</td>\n",
              "      <td>[tf] cara hapus tag contact di get contact yan...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16/01/2023 09:41</td>\n",
              "      <td>BSI_Tegal</td>\n",
              "      <td>Selamat Bertugas kepada peserta BSI Explore ya...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16/01/2023 09:39</td>\n",
              "      <td>damkarbekasi</td>\n",
              "      <td>Minggu, 15 Januari 2023 petugas Pleton C Regu ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               date      username  \\\n",
              "0  16/01/2023 10:08     tegalfess   \n",
              "1  16/01/2023 09:56     tegalfess   \n",
              "2  16/01/2023 09:47     tegalfess   \n",
              "3  16/01/2023 09:41     BSI_Tegal   \n",
              "4  16/01/2023 09:39  damkarbekasi   \n",
              "\n",
              "                                               tweet  wisata/hiburan  \\\n",
              "0  [tf] yg jual gini area slawi aja dimana ya ges...              -1   \n",
              "1  [tf] spill game offline kalian yg menarik itu ...               1   \n",
              "2  [tf] cara hapus tag contact di get contact yan...              -1   \n",
              "3  Selamat Bertugas kepada peserta BSI Explore ya...              -1   \n",
              "4  Minggu, 15 Januari 2023 petugas Pleton C Regu ...              -1   \n",
              "\n",
              "   pendidikan  fasilitas/layanan publik  kuliner  \n",
              "0          -1                        -1       -1  \n",
              "1          -1                        -1       -1  \n",
              "2          -1                        -1       -1  \n",
              "3           1                        -1       -1  \n",
              "4          -1                         1       -1  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_aspek.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aspek Wisata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tchu-uO0eBdT"
      },
      "outputs": [],
      "source": [
        "#Panggil Data target aspek (y)\n",
        "label_aspek_1 = data_aspek['wisata/hiburan']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aspek Pendidikan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_aspek_2 = data_aspek['pendidikan']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aspek Fasilitas dan Layanan Public"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_aspek_3 = data_aspek['fasilitas/layanan publik']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aspek Kuliner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_aspek_4 = data_aspek['kuliner']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "W3VZKn6s5Q8c"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(vektor_tfidf, label_aspek_4, test_size=0.2)#, shuffle=True)\n",
        "nama = \"Model_Aspek_Kuliner\"\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMIADKf2vezg",
        "outputId": "e1a2bc80-778b-4d3f-fdb1-84e999d0c476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1\n",
            "  1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1  1 -1 -1 -1  1  1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  1 -1 -1 -1  1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1  1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1  1 -1 -1\n",
            " -1 -1 -1 -1  1 -1 -1 -1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 24)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(y_test)\n",
        "p = np.where(y_test==1)\n",
        "np.array(p).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r83boEheVfpQ",
        "outputId": "f118a835-ece2-42e1-83b8-9546f722e9f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n"
          ]
        }
      ],
      "source": [
        "# Cross-validasi\n",
        "kf = StratifiedKFold(n_splits=2)\n",
        "kf.get_n_splits(X_train) \n",
        "print(kf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTAtIGpRxkl8"
      },
      "source": [
        "# 5.Training Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b1hyzyBH8FsB"
      },
      "outputs": [],
      "source": [
        "# fungsi confusion matrix\n",
        "def hitung_score_nvb(y_test, y_pred):\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
        "    print(\" True Positif : \" + str(tp),\n",
        "          \"True Negatif : \" + str(tn), \n",
        "          \"False Positif : \" + str(fp), \n",
        "          \"False Negatif : \" + str(fn) + '\\n')\n",
        "    # print(fp, tp)\n",
        "    acc_nvb = (tp+tn) / (tp + fp + tn + fn)\n",
        "    prec_nvb = tp / (tp + fp)\n",
        "    recc_nvb = tp / (tp + fn)\n",
        "    f_1_nvb = 2 * (prec_nvb * recc_nvb) / (prec_nvb + recc_nvb)\n",
        "    return acc_nvb, prec_nvb, recc_nvb, f_1_nvb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9uAqeeyCjgwZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.multiclass import unique_labels\n",
        "def plot(test_y, pred_y):\n",
        "  labels = unique_labels(y_test)\n",
        "  column = [f'Prediksi {label}' for label in labels]\n",
        "  indeces = [f'Actual {label}' for label in labels]\n",
        "  table = pd.DataFrame(confusion_matrix(test_y, pred_y),\n",
        "                       columns = column, index = indeces)\n",
        "  return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "cBWZ_WB44w_g"
      },
      "outputs": [],
      "source": [
        "# print(plot(label_asli, pred_y_nvb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SS3vD4y4f9g",
        "outputId": "1b8af1d8-62e7-4051-bed3-8d306f64a266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "confusion matrix : Model_Aspek_Kuliner\n",
            "Fold ke-1\n",
            "--------------------------------------------- \n",
            "\n",
            "           Prediksi -1  Prediksi 0  Prediksi 1\n",
            "Actual -1          323           0           0\n",
            "Actual 0             1           0           0\n",
            "Actual 1            30           0           0 \n",
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [48], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mprint\u001b[39m(plot(label_asli, pred_y_nvb),\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[39m# print(confusion_matrix(label_asli, pred_y_nvb))\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m acc, prec, recc, f_1 \u001b[39m=\u001b[39m hitung_score_nvb(label_asli, pred_y_nvb)\n\u001b[0;32m     43\u001b[0m \u001b[39m# print(prediction[:9])\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m# print(y_test_aspek_nvb[:9])\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mScore fold ke-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(kf_nvb_number),\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn [45], line 3\u001b[0m, in \u001b[0;36mhitung_score_nvb\u001b[1;34m(y_test, y_pred)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhitung_score_nvb\u001b[39m(y_test, y_pred):\n\u001b[1;32m----> 3\u001b[0m     tn, fp, fn, tp \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mconfusion_matrix(y_test, y_pred)\u001b[39m.\u001b[39mravel()\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m True Positif : \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(tp),\n\u001b[0;32m      5\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTrue Negatif : \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(tn), \n\u001b[0;32m      6\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mFalse Positif : \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(fp), \n\u001b[0;32m      7\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mFalse Negatif : \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(fn) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[39m# print(fp, tp)\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ],
      "source": [
        "kf_nvb_number = 0\n",
        "akurasi_nvb_arr = []\n",
        "prec_nvb_arr = []\n",
        "rec_nvb_arr = []\n",
        "f1_nvb_arr = []\n",
        "score = {\n",
        "    'Akurasi':[],\n",
        "    'Presisi':[],\n",
        "    'Recall':[],\n",
        "    'F1':[]\n",
        "}\n",
        "index_fold = 1\n",
        "\n",
        "for train_aspek_nvb, test_aspek_nvb in kf.split(X_train, y_train):\n",
        "  X_train_aspek_nvb, X_test_aspek_nvb = X_train[train_aspek_nvb], X_train[test_aspek_nvb]\n",
        "  y_train_aspek_nvb, y_test_aspek_nvb = y_train[train_aspek_nvb], y_train[test_aspek_nvb]\n",
        " \n",
        "  kf_nvb_number += 1\n",
        " \n",
        "  # train model\n",
        "  clf = MultinomialNB()\n",
        "  clf.fit(X_train_aspek_nvb, y_train_aspek_nvb)\n",
        " \n",
        "  # save model\n",
        "  # model_nvb = 'tfidf_sentimen_integritas_nvb.sav'\n",
        "  pickle.dump(clf, open('Model/tfidf_{}_nvb.pkl'.format(nama), 'wb'))\n",
        " \n",
        "  # pengujian\n",
        "  pred_y_nvb = clf.predict(X_test_aspek_nvb)\n",
        "  label_asli = y_test_aspek_nvb\n",
        "  \n",
        "  print('\\n'+'-'*45)\n",
        "  print('confusion matrix : ' + str(nama)+'\\n' + 'Fold ke-' + str(kf_nvb_number))\n",
        "  print('-'*45,'\\n')\n",
        "\n",
        "  # print('confusion matrix :')\n",
        "  # print('-'*27,'\\n')\n",
        "  print(plot(label_asli, pred_y_nvb),'\\n')\n",
        "\n",
        "  # print(confusion_matrix(label_asli, pred_y_nvb))\n",
        "  acc, prec, recc, f_1 = hitung_score_nvb(label_asli, pred_y_nvb)\n",
        "\n",
        "  # print(prediction[:9])\n",
        "  # print(y_test_aspek_nvb[:9])\n",
        " \n",
        "  print(\"Score fold ke-\" + str(kf_nvb_number),'\\n')\n",
        "  print(\"Naive Bayes Accuracy Score -> \", str(float(\"%.2f\" % round(accuracy_score(pred_y_nvb, y_test_aspek_nvb)*100, 2))) + \" %\")\n",
        "  print(\"Naive Bayes Precission Score -> \", str(float(\"%.2f\" % round(precision_score(pred_y_nvb, y_test_aspek_nvb, average='macro')*100, 2)))+ \" %\")\n",
        "  print(\"Naive Bayes Recall Score -> \", str(float(\"%.2f\" % round(recall_score(pred_y_nvb, y_test_aspek_nvb, average='macro')*100, 2)))+ \" %\")\n",
        "  print(\"Naive Bayes F1 Measurement Score -> \", str(float(\"%.2f\" % round(f1_score(pred_y_nvb, y_test_aspek_nvb, average='macro')*100, 2)))+ \" %\" + \"\\n\")\n",
        "  \n",
        "  # print(\"confusion matrix :\")\n",
        "  # print(confusion_matrix(label_asli, pred_y_nvb))\n",
        "  # print(\"\\n\",\"True positif : \" + str(cm[1][1]), \"\\n\",\n",
        "  #       \"True negatif : \" + str(cm[0][0]), \"\\n\",\n",
        "  #       \"False positif : \" + str(cm[0][1]), \"\\n\",\n",
        "  #       \"False negatif : \" + str(cm[1][0]))\n",
        "  index_fold += 1\n",
        "\n",
        "  score['Akurasi'].append(acc)\n",
        "  score['Presisi'].append(prec)\n",
        "  score['Recall'].append(recc)\n",
        "  score['F1'].append(f_1)\n",
        "  akurasi_nvb_arr.append(accuracy_score(pred_y_nvb, y_test_aspek_nvb))\n",
        "  prec_nvb_arr.append(precision_score(pred_y_nvb, y_test_aspek_nvb,average='macro'))\n",
        "  rec_nvb_arr.append(recall_score(pred_y_nvb, y_test_aspek_nvb,average='macro'))\n",
        "  f1_nvb_arr.append(f1_score(pred_y_nvb, y_test_aspek_nvb,average='macro'))\n",
        "  \n",
        "  print('-'*100,'\\n')\n",
        "# from termcolor import colored\n",
        "# print(colored(\"Rata-rata Akurasi NVB : \" + str(round(np.mean(akurasi_nvb_arr)*100,2, 'red', attrs=['bold'])),\"%\")\n",
        "print(\"Rata-rata Akurasi NVB : \" + str(round(np.mean(akurasi_nvb_arr)*100,2)), \"%\")\n",
        "print(\"Precision K-Fold : \" + str(round(np.mean(prec_nvb_arr)*100,2)), \"%\")\n",
        "print(\"Recall K-Fold : \" + str(round(np.mean(rec_nvb_arr)*100,2)), \"%\")\n",
        "print(\"F1 K-Fold : \" + str(round(np.mean(f1_nvb_arr)*100,2)), \"%\")\n",
        "print(\"Fold-1 : \" +str(score['Akurasi'][0]) + \"%\", \n",
        "      \"Fold-2 : \" + str(score['Akurasi'][1]) + \"%\",\n",
        "      \"Fold-3 : \" + str(score['Akurasi'][2]) + \"%\", \n",
        "      \"Fold-4 : \" + str(score['Akurasi'][3]) + \"%\") \n",
        "      #\"Fold-5 : \" + str(score['Akurasi'][4]) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuxSa8LO4OUv"
      },
      "source": [
        "# 6.Pengujian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyD7QcrqLFfP",
        "outputId": "49300766-3973-4d87-9e93-c221a7aa6803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------\n",
            "confusion matrix : aspek_aksesbilitas_v3\n",
            "--------------------------------------------- \n",
            "\n",
            "          Prediksi 0  Prediksi 1\n",
            "Actual 0        1157           0\n",
            "Actual 1         208           0 \n",
            "\n",
            " True Positif : 0 True Negatif : 1157 False Positif : 0 False Negatif : 208\n",
            "\n",
            "\u001b[1m\u001b[31mPengujian aspek_aksesbilitas_v3: \u001b[0m\n",
            "Model Terbaik Fold ke-4 \n",
            " Akurasi  : 84.76% \n",
            " Presisi  : nan% \n",
            " Recall  : 0.0% \n",
            " F1 Score  : nan% \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ],
      "source": [
        "indeks=np.where(score[\"Akurasi\"]==np.amax(score['Akurasi']))[0]\n",
        "\n",
        "model = pickle.load(open('/content/drive/MyDrive/1 Rizki Folder/Hibah bersama Dosen/result_baru/nvb/tfidf_'+str(nama)+'_nvb.pkl','rb'))\n",
        "pred_y_ = model.predict(X_test)\n",
        "label_test = y_test\n",
        "# result = model.score(X_test, y_test)\n",
        "print('\\n'+'-'*45)\n",
        "print('confusion matrix : ' + str(nama))\n",
        "print('-'*45,'\\n')\n",
        "print(plot(label_test, pred_y_),'\\n')\n",
        "# print(\"Confusion Matrix :\")\n",
        "# print('-'*20,'\\n')\n",
        "\n",
        "# print(confusion_matrix(label_test, pred_y_))\n",
        "acc, prec, recc, f_1 = hitung_score_nvb(label_test, pred_y_)\n",
        "from termcolor import colored\n",
        "print(colored(\"Pengujian \" +str(nama) + ': ',  'red', attrs=['bold']))\n",
        "print('Model Terbaik Fold ke-' +str(indeks[0]+1, ),'\\n',\n",
        "      'Akurasi  : ' + str(round((acc*100) ,2)) + \"%\",\"\\n\",\n",
        "      'Presisi  : '+ str(round((prec*100) ,2))+ \"%\",\"\\n\", \n",
        "      'Recall  : '+ str(round((recc*100) ,2))+ \"%\",\"\\n\", \n",
        "      'F1 Score  : '+ str(round((f_1*100) ,2))+ \"%\",'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL3y9X5-lJdf",
        "outputId": "4a4e6b0a-ee82-4767-efa8-f1bb6f0f41ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lCyZrucmK2a"
      },
      "source": [
        "# Augmentasi data \n",
        "Bisa menggunakan Smote, adasyn, dan randomoversampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6k6vXDQltVf",
        "outputId": "673b5290-d51c-4af4-e0eb-a088d3f7a69b"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 6",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [59], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Contoh menggunakan Adasyn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTE, ADASYN\n\u001b[1;32m----> 3\u001b[0m X_resampled, y_resampled \u001b[39m=\u001b[39m ADASYN()\u001b[39m.\u001b[39;49mfit_resample(X_train, y_train)\n",
            "File \u001b[1;32md:\\Materi Kuliah\\SKRIPSI\\skripsiSentiment\\lib\\site-packages\\imblearn\\base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 203\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
            "File \u001b[1;32md:\\Materi Kuliah\\SKRIPSI\\skripsiSentiment\\lib\\site-packages\\imblearn\\base.py:88\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     82\u001b[0m X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_ \u001b[39m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampling_type\n\u001b[0;32m     86\u001b[0m )\n\u001b[1;32m---> 88\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_resample(X, y)\n\u001b[0;32m     90\u001b[0m y_ \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m     label_binarize(output[\u001b[39m1\u001b[39m], classes\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39munique(y)) \u001b[39mif\u001b[39;00m binarize_y \u001b[39melse\u001b[39;00m output[\u001b[39m1\u001b[39m]\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     94\u001b[0m X_, y_ \u001b[39m=\u001b[39m arrays_transformer\u001b[39m.\u001b[39mtransform(output[\u001b[39m0\u001b[39m], y_)\n",
            "File \u001b[1;32md:\\Materi Kuliah\\SKRIPSI\\skripsiSentiment\\lib\\site-packages\\imblearn\\over_sampling\\_adasyn.py:202\u001b[0m, in \u001b[0;36mADASYN._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39m# the nearest neighbors need to be fitted only on the current class\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39m# to find the class NN to generate new samples\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_\u001b[39m.\u001b[39mfit(X_class)\n\u001b[1;32m--> 202\u001b[0m nns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnn_\u001b[39m.\u001b[39;49mkneighbors(X_class, return_distance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[:, \u001b[39m1\u001b[39m:]\n\u001b[0;32m    204\u001b[0m enumerated_class_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(target_class_indices))\n\u001b[0;32m    205\u001b[0m rows \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrepeat(enumerated_class_indices, n_samples_generate)\n",
            "File \u001b[1;32md:\\Materi Kuliah\\SKRIPSI\\skripsiSentiment\\lib\\site-packages\\sklearn\\neighbors\\_base.py:810\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    808\u001b[0m n_samples_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_fit_\n\u001b[0;32m    809\u001b[0m \u001b[39mif\u001b[39;00m n_neighbors \u001b[39m>\u001b[39m n_samples_fit:\n\u001b[1;32m--> 810\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    811\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected n_neighbors <= n_samples, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but n_samples = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, n_neighbors = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[0;32m    813\u001b[0m     )\n\u001b[0;32m    815\u001b[0m n_jobs \u001b[39m=\u001b[39m effective_n_jobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n\u001b[0;32m    816\u001b[0m chunked_results \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 6"
          ]
        }
      ],
      "source": [
        "# Contoh menggunakan Adasyn\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "X_resampled, y_resampled = ADASYN().fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "2PncGWNHm-aJ",
        "outputId": "34f5619d-b86d-4a13-c4ed-092a58721d31"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-ea03446c01fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# contoh menggunakan Smote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# contoh menggunakan Smote\n",
        "X_resampled, y_resampled = SMOTE().fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PZorBpTiinx",
        "outputId": "008f8f7d-80cc-487c-9be0-585b4b8ce606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 4354), (1, 4354)]\n"
          ]
        }
      ],
      "source": [
        "# contoh menggunakan RandomOversampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_resampled, y_resampled = ros.fit_resample(vektor_tfidf, label_aspek_atraksi)\n",
        "from collections import Counter\n",
        "print(sorted(Counter(y_resampled).items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjQscsQji1Nf",
        "outputId": "426280e0-d3f8-45ce-9990-763787174a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9297197978870004\n",
            "0.9127239320165366\n",
            "0.9448407037565383\n",
            "0.9285046728971963\n",
            "[2573 2697  562 ... 3821 6071 1067]\n"
          ]
        }
      ],
      "source": [
        "clf = MultinomialNB()\n",
        "clf.fit(X_resampled, y_resampled)\n",
        "pred = clf.predict(X_resampled)\n",
        "print(clf.score(X_resampled, y_resampled))\n",
        "print(precision_score(pred, y_resampled))\n",
        "print(recall_score(pred, y_resampled))\n",
        "print(f1_score(pred, y_resampled))\n",
        "print(X_resampled.argmax(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHdO8wfprbfY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gNzxbK5i0Pcu",
        "8A0nfLDO0Xgy",
        "Ts2lHtue0kAk",
        "UZk9kLGa0sSB"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "skripsiSentimet",
      "language": "python",
      "name": "skripsisentimet"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
