{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-z7UFhKeRSK"
      },
      "source": [
        "# 1.Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G0_9LY0xaEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54437fd-2ed4-4da8-bfd3-2e50072b6a2e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uBcsL0kBoJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2469be-c166-4af4-dfac-9a707d63e57c"
      },
      "source": [
        "!pip install varname"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting varname\n",
            "  Downloading varname-0.9.1-py3-none-any.whl (22 kB)\n",
            "Collecting executing<2.0,>=1.0\n",
            "  Downloading executing-1.1.0-py2.py3-none-any.whl (22 kB)\n",
            "Installing collected packages: executing, varname\n",
            "Successfully installed executing-1.1.0 varname-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUbsTDyZH73U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc30ffb-a89e-4d81-b8c1-ea50f57ffcdb"
      },
      "source": [
        "!pip install keras-metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-metrics\n",
            "  Downloading keras_metrics-1.1.0-py2.py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.7/dist-packages (from keras-metrics) (2.8.0)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kZUFQ0Dtk1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a03227d-a5c1-4562-916f-75eb533a13a5"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI4ujUlnZah4"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import multiprocessing\n",
        "import io\n",
        "import gensim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras_metrics as km\n",
        "import pickle\n",
        "import keras\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Activation, Embedding, LSTM, Bidirectional, Dropout, GRU\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import defaultdict\n",
        "from datetime import timedelta\n",
        "from gensim.models import word2vec\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from varname import nameof"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtyarzfKc1DO"
      },
      "source": [
        "#2.Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewFARWiLXQBy"
      },
      "source": [
        "path_data = \"/content/drive/MyDrive/2 Aji Folder/dataset sentimen.csv\"\n",
        "data = pd.read_csv(path_data, sep=\";\", header=[0], encoding=\"UTF-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC-dodrjDpYK"
      },
      "source": [
        "# 3.Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1AAgy-u1byv"
      },
      "source": [
        "# panggil data target sentimen (y)\n",
        "label_atraksi = data['Atraksi']\n",
        "label_aksesbilitas = data['Aksesbilitas']\n",
        "label_fasilitas = data['Fasilitas']\n",
        "label_akomodasi = data['Akomodasi ']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWjPsi51f38o"
      },
      "source": [
        "reviews = pickle.load(open(\"/content/drive/MyDrive/2 Aji Folder/reviews.pickle\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aznz_44BghiM"
      },
      "source": [
        "# pembuatan vector kata\n",
        "reviews2 = [\" \".join(r) for r in reviews]\n",
        "vectorizer = TfidfVectorizer()\n",
        "vektor_tfidf = vectorizer.fit_transform(reviews2)\n",
        "vektor_tfidf = vektor_tfidf.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnv7NasUgbl-"
      },
      "source": [
        "# reviews2 = [\" \".join(r) for r in reviews]\n",
        "X_train, X_test, y_train, y_test = train_test_split(vektor_tfidf, label_aksesbilitas, test_size=0.2)\n",
        "nama = \"aspek_aksesbilitas\"\n",
        "nama_sentimen = \"sentimen_aksesbilitas\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6hOvvkkZ1lc",
        "outputId": "74e5e3a9-5f66-427c-f0ce-98ba97a8e7e2"
      },
      "source": [
        "# print(y_train,y_test)\n",
        "p = np.where(y_test==1)\n",
        "print(np.array(p).shape)\n",
        "#labels = unique_labels(y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 102)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA1zD3l2qMpV"
      },
      "source": [
        "# 4.Load Model Aspek & prediksi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P64WuKAMnMl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "bb10f8b4-7c39-4c00-fc24-622538349c3a"
      },
      "source": [
        "model_aspek = pickle.load(open('/content/drive/MyDrive/1 Rizki Folder/Hibah bersama Dosen/result_baru/nvb/tfidf_'+str(nama)+'_nvb.pkl','rb'))\n",
        "model_sentimen = pickle.load(open('/content/drive/MyDrive/2 Aji Folder/result/nvb/tfidf_'+str(nama_sentimen)+'_nvb.pkl','rb'))\n",
        "\n",
        "result = []\n",
        "\n",
        "for test in X_test:\n",
        "  pred = model_aspek.predict(np.expand_dims(test,axis=0))\n",
        "  if pred != 1:\n",
        "    result.append(-1)\n",
        "  else:\n",
        "    pred = model_sentimen.predict(np.expand_dims(test,axis=0))\n",
        "    result.append(pred[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-98db197faf98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_aspek\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \"\"\"\n\u001b[1;32m     81\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;34m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             raise ValueError(\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: X has 10484 features, but MultinomialNB is expecting 7235 features as input."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyBXX0JwVeHl",
        "outputId": "566e2506-faac-4383-a4cd-2f184287bb1e"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1wxW7OOC4cs"
      },
      "source": [
        "# 5.Pengujian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWzsiBFnVh5s"
      },
      "source": [
        "from sklearn.utils.multiclass import unique_labels\n",
        "def plot(test_y, result):\n",
        "  labels = unique_labels(y_test)\n",
        "  column = [f'Prediksi {label}' for label in labels]\n",
        "  indeces = [f'Actual {label}' for label in labels]\n",
        "  table = pd.DataFrame(confusion_matrix(test_y, result),\n",
        "                       columns = column, index = indeces)\n",
        "  return table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAXl9RwOOQp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d86d43-a6c8-4183-9834-e8fdc5ac383e"
      },
      "source": [
        "def hitung_score_multiclass(y_test, y_pred):\n",
        "    cm = metrics.confusion_matrix(y_test, y_pred,labels=[-1,0,1])\n",
        "    # print(cm)\n",
        "    print('\\n'+'-'*80)\n",
        "    print('confusion matrix : ' + str(nama_aspek) + ' & ' + str(nama_sentimen) + ' secara sekuensial')\n",
        "    print('-'*80,'\\n')\n",
        "    print(plot(y_test, result),'\\n')\n",
        "    total_true = np.sum(cm, axis=1)\n",
        "    total_pred = np.sum(cm, axis=0)\n",
        "    tp_a = cm[0][0]\n",
        "    tp_b = cm[1][1]\n",
        "    tp_c = cm[2][2]\n",
        "    st_a = total_true[0]\n",
        "    st_b = total_true[1]\n",
        "    st_c = total_true[2]\n",
        "    sp_a = total_pred[0]\n",
        "    sp_b = total_pred[1]\n",
        "    sp_c = total_pred[2]\n",
        "    print(\"total true : \" +str(total_true))\n",
        "    print(\"total pred : \" +str(total_pred))\n",
        "    print(\"tp_a : \" + str(tp_a), \n",
        "          \"tp_b : \" + str(tp_b), \n",
        "          \"tp_c : \" + str(tp_c), \n",
        "          \"st_a : \" + str(st_a),\n",
        "          \"st_b : \" + str(st_b),\n",
        "          \"st_c : \" + str(st_c),\n",
        "          \"sp_a : \" + str(sp_a),\n",
        "          \"sp_b : \" + str(sp_b),\n",
        "          \"sp_c : \" + str(sp_c),'\\n')\n",
        "    \n",
        "    acc = (tp_a + tp_b + tp_c) / np.sum(total_true) * 100\n",
        "    prec = (tp_c / sp_c) * 100\n",
        "    recc = (tp_c / st_c) * 100\n",
        "    f1 = 2 * (prec * recc) / (prec + recc)\n",
        "    from termcolor import colored\n",
        "    print(colored(\"Pengujian :\", 'red', attrs=['bold']),'\\n',\n",
        "      'Akurasi  : ' + str(acc) + \"%\",\"\\n\",\n",
        "      'Presisi  : '+ str(prec)+ \"%\",\"\\n\", \n",
        "      'Recall  : '+ str(recc)+ \"%\",\"\\n\", \n",
        "      'F1 Score  : '+ str(f1)+ \"%\",'\\n')\n",
        "    return  round(acc,2) , round(prec,2), round(recc,2), round(f1,2)\n",
        "\n",
        "print(hitung_score_multiclass(y_test,result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "confusion matrix : aspek_kontinuitas & sentimen_kontinuitas secara sekuensial\n",
            "-------------------------------------------------------------------------------- \n",
            "\n",
            "           Prediksi -1  Prediksi 0  Prediksi 1\n",
            "Actual -1          266           0           0\n",
            "Actual 0             2           0           0\n",
            "Actual 1             1           0           0 \n",
            "\n",
            "total true : [266   2   1]\n",
            "total pred : [269   0   0]\n",
            "tp_a : 266 tp_b : 0 tp_c : 0 st_a : 266 st_b : 2 st_c : 1 sp_a : 269 sp_b : 0 sp_c : 0 \n",
            "\n",
            "\u001b[1m\u001b[31mPengujian :\u001b[0m \n",
            " Akurasi  : 98.88475836431226% \n",
            " Presisi  : nan% \n",
            " Recall  : 0.0% \n",
            " F1 Score  : nan% \n",
            "\n",
            "(98.88, nan, 0.0, nan)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in long_scalars\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GB6bPjp1iPO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}